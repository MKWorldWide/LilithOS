"""
ðŸ¤– Base AI Model Interface

This module defines the base interface for all AI model integrations in the
TrafficFlou system, providing a consistent API for different AI providers.

Author: TrafficFlou Team
Version: 1.0.0
"""

import asyncio
import time
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum

import structlog
from pydantic import BaseModel, Field

from ..core.exceptions import AIModelError


class ModelType(Enum):
    """Enumeration of supported AI model types."""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    ATHENA = "athena"
    PRIMAL_GENESIS = "primal_genesis"
    CUSTOM = "custom"


class BehaviorType(Enum):
    """Enumeration of behavior types that AI models can generate."""
    BROWSING = "browsing"
    CLICKING = "clicking"
    SCROLLING = "scrolling"
    FORM_FILLING = "form_filling"
    NAVIGATION = "navigation"
    SEARCH = "search"
    SOCIAL = "social"
    ECOMMERCE = "ecommerce"


@dataclass
class BehaviorPattern:
    """Represents a single behavior pattern generated by an AI model."""
    
    behavior_type: BehaviorType
    target_element: Optional[str] = None
    action: str = ""
    parameters: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 1.0
    reasoning: str = ""
    timestamp: float = field(default_factory=time.time)


@dataclass
class AIModelResponse:
    """Represents a response from an AI model."""
    
    model_name: str
    behavior_patterns: List[BehaviorPattern]
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    tokens_used: Optional[int] = None
    cost: Optional[float] = None
    success: bool = True
    error_message: Optional[str] = None


class AIModelConfig(BaseModel):
    """Base configuration for AI models."""
    
    model_name: str = Field(..., description="Name of the AI model")
    api_key: str = Field(..., description="API key for the model provider")
    max_tokens: int = Field(default=1000, description="Maximum tokens per request")
    temperature: float = Field(default=0.7, description="Model temperature")
    timeout: int = Field(default=30, description="Request timeout in seconds")
    retry_attempts: int = Field(default=3, description="Number of retry attempts")
    retry_delay: float = Field(default=1.0, description="Delay between retries in seconds")
    rate_limit: Optional[int] = Field(default=None, description="Requests per minute limit")
    
    # Behavior generation specific settings
    behavior_types: List[BehaviorType] = Field(
        default=[BehaviorType.BROWSING, BehaviorType.CLICKING, BehaviorType.SCROLLING],
        description="Types of behaviors to generate"
    )
    max_patterns_per_request: int = Field(default=5, description="Maximum behavior patterns per request")
    confidence_threshold: float = Field(default=0.7, description="Minimum confidence for behavior patterns")


class BaseAIModel(ABC):
    """
    ðŸ¤– Base AI Model Interface
    
    Abstract base class that defines the interface for all AI model integrations
    in the TrafficFlou system. Provides consistent methods for behavior generation,
    error handling, and performance monitoring.
    
    Attributes:
        model_name (str): Name of the AI model
        model_type (ModelType): Type of the AI model
        config (AIModelConfig): Model configuration
        logger (structlog.BoundLogger): Structured logging instance
        request_count (int): Total number of requests made
        error_count (int): Total number of errors encountered
        total_processing_time (float): Total processing time across all requests
    """
    
    def __init__(self, model_name: str, config: AIModelConfig):
        """
        Initialize the base AI model.
        
        Args:
            model_name (str): Name of the AI model
            config (AIModelConfig): Model configuration
        """
        self.model_name = model_name
        self.config = config
        self.logger = structlog.get_logger(__name__)
        
        # Performance tracking
        self.request_count = 0
        self.error_count = 0
        self.total_processing_time = 0.0
        self.last_request_time = 0.0
        
        # Rate limiting
        self._rate_limit_tokens = 0
        self._rate_limit_last_reset = time.time()
        
        self.logger.info("AI model initialized", 
                        model_name=model_name,
                        model_type=self.model_type)
    
    @property
    @abstractmethod
    def model_type(self) -> ModelType:
        """Return the type of this AI model."""
        pass
    
    @abstractmethod
    async def _make_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        Make a request to the AI model API.
        
        Args:
            prompt (str): Input prompt for the model
            **kwargs: Additional request parameters
            
        Returns:
            Dict[str, Any]: Raw response from the model API
            
        Raises:
            AIModelError: If the request fails
        """
        pass
    
    @abstractmethod
    def _parse_response(self, response: Dict[str, Any]) -> List[BehaviorPattern]:
        """
        Parse the raw API response into behavior patterns.
        
        Args:
            response (Dict[str, Any]): Raw response from the model API
            
        Returns:
            List[BehaviorPattern]: Parsed behavior patterns
        """
        pass
    
    async def generate_behavior(
        self, 
        target_url: str, 
        behavior_patterns: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AIModelResponse:
        """
        Generate intelligent behavior patterns for traffic simulation.
        
        Args:
            target_url (str): Target website URL
            behavior_patterns (Dict[str, Any]): Base behavior patterns
            context (Dict[str, Any]): Additional context information
            
        Returns:
            AIModelResponse: Generated behavior patterns and metadata
        """
        start_time = time.time()
        
        try:
            # Check rate limits
            await self._check_rate_limit()
            
            # Prepare prompt
            prompt = self._prepare_prompt(target_url, behavior_patterns, context)
            
            # Make request to AI model
            raw_response = await self._make_request(prompt)
            
            # Parse response
            behavior_patterns_list = self._parse_response(raw_response)
            
            # Update performance metrics
            processing_time = time.time() - start_time
            self._update_metrics(processing_time, success=True)
            
            return AIModelResponse(
                model_name=self.model_name,
                behavior_patterns=behavior_patterns_list,
                processing_time=processing_time,
                tokens_used=raw_response.get('usage', {}).get('total_tokens'),
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_metrics(processing_time, success=False)
            
            self.logger.error("Behavior generation failed", 
                            model_name=self.model_name,
                            error=str(e))
            
            return AIModelResponse(
                model_name=self.model_name,
                behavior_patterns=[],
                processing_time=processing_time,
                success=False,
                error_message=str(e)
            )
    
    def _prepare_prompt(
        self, 
        target_url: str, 
        behavior_patterns: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Prepare the prompt for the AI model.
        
        Args:
            target_url (str): Target website URL
            behavior_patterns (Dict[str, Any]): Base behavior patterns
            context (Dict[str, Any]): Additional context
            
        Returns:
            str: Formatted prompt for the AI model
        """
        prompt_template = self._get_prompt_template()
        
        # Extract relevant information from behavior patterns
        page_elements = behavior_patterns.get('page_elements', [])
        user_intent = behavior_patterns.get('user_intent', 'browsing')
        session_duration = behavior_patterns.get('session_duration', 300)
        
        # Build context information
        context_info = {
            'target_url': target_url,
            'page_elements': page_elements,
            'user_intent': user_intent,
            'session_duration': session_duration,
            'behavior_types': [bt.value for bt in self.config.behavior_types]
        }
        
        if context:
            context_info.update(context)
        
        # Format the prompt
        return prompt_template.format(**context_info)
    
    def _get_prompt_template(self) -> str:
        """Get the prompt template for behavior generation."""
        return """
You are an AI assistant that generates realistic user behavior patterns for web traffic simulation.

Target URL: {target_url}
User Intent: {user_intent}
Session Duration: {session_duration} seconds
Available Page Elements: {page_elements}
Behavior Types to Generate: {behavior_types}

Please generate realistic behavior patterns that a human user would perform on this website.
Each behavior should include:
1. Behavior type (browsing, clicking, scrolling, form_filling, navigation, search, social, ecommerce)
2. Target element (if applicable)
3. Action description
4. Parameters (coordinates, duration, etc.)
5. Confidence level (0.0-1.0)
6. Reasoning for the behavior

Generate {max_patterns_per_request} behavior patterns that mimic organic user behavior.
Focus on realistic interactions that would occur naturally during a {session_duration}-second session.

Response format (JSON):
{{
    "behaviors": [
        {{
            "type": "behavior_type",
            "target_element": "element_selector_or_description",
            "action": "detailed_action_description",
            "parameters": {{"param1": "value1"}},
            "confidence": 0.85,
            "reasoning": "explanation_of_why_this_behavior_is_realistic"
        }}
    ]
}}
"""
    
    async def _check_rate_limit(self):
        """Check and enforce rate limits."""
        if not self.config.rate_limit:
            return
        
        current_time = time.time()
        
        # Reset tokens if a minute has passed
        if current_time - self._rate_limit_last_reset >= 60:
            self._rate_limit_tokens = 0
            self._rate_limit_last_reset = current_time
        
        # Check if we're at the limit
        if self._rate_limit_tokens >= self.config.rate_limit:
            wait_time = 60 - (current_time - self._rate_limit_last_reset)
            if wait_time > 0:
                self.logger.warning("Rate limit reached, waiting", 
                                  wait_time=wait_time,
                                  model_name=self.model_name)
                await asyncio.sleep(wait_time)
                self._rate_limit_tokens = 0
                self._rate_limit_last_reset = time.time()
        
        self._rate_limit_tokens += 1
    
    def _update_metrics(self, processing_time: float, success: bool):
        """Update performance metrics."""
        self.request_count += 1
        self.total_processing_time += processing_time
        self.last_request_time = time.time()
        
        if not success:
            self.error_count += 1
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for this model."""
        avg_processing_time = (
            self.total_processing_time / self.request_count 
            if self.request_count > 0 else 0
        )
        
        error_rate = (
            self.error_count / self.request_count 
            if self.request_count > 0 else 0
        )
        
        return {
            "model_name": self.model_name,
            "model_type": self.model_type.value,
            "request_count": self.request_count,
            "error_count": self.error_count,
            "error_rate": error_rate,
            "total_processing_time": self.total_processing_time,
            "average_processing_time": avg_processing_time,
            "last_request_time": self.last_request_time
        }
    
    async def health_check(self) -> bool:
        """
        Perform a health check on the AI model.
        
        Returns:
            bool: True if the model is healthy, False otherwise
        """
        try:
            # Make a simple test request
            test_prompt = "Generate a simple browsing behavior pattern."
            response = await self._make_request(test_prompt)
            
            # Check if response is valid
            if response and 'choices' in response:
                return True
            else:
                return False
                
        except Exception as e:
            self.logger.error("Health check failed", 
                            model_name=self.model_name,
                            error=str(e))
            return False
    
    async def close(self):
        """Clean up resources used by the model."""
        self.logger.info("Closing AI model", model_name=self.model_name)
        # Subclasses can override this to clean up specific resources 